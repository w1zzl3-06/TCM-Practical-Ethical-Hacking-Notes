# 20. Web Application Enumeration, Revisited.

### I. Installing Go

To install Go. All we need to do is run [pimpmykali](https://github.com/Dewalt-arch/pimpmykali) with `./pimpmykali`, and select option 3:

![pimpmykali.png](20%20Web%20Application%20Enumeration,%20Revisited%20bfecbe21ba6b4faeb558d14d84e57a30/pimpmykali.png)

The reason we are installing the go language is because all the tools in this section will require it.

### II. Finding Subdomains with AssetFinder

Find the tool here: 

[https://github.com/tomnomnom/assetfinder](https://github.com/tomnomnom/assetfinder)

Follow the installation steps in the repo(once again, make sure go is installed).

Now let’s use it to scan for subdomains using `assestfinder <URL>`:

![assetfinder 1.png](20%20Web%20Application%20Enumeration,%20Revisited%20bfecbe21ba6b4faeb558d14d84e57a30/assetfinder_1.png)

This will produce a lot of results, we will trim it down later.

We can also output it to a file with `assetfinder <URL> >> file.txt`

To find specific domains tied to only the domain: `assetfinder --subs-only <URL>`:

![assetfinder 2.png](20%20Web%20Application%20Enumeration,%20Revisited%20bfecbe21ba6b4faeb558d14d84e57a30/assetfinder_2.png)

One of the best ways to make looking through the subdomains easier is to use the grep command after. We can write a tool that can do just that.

Open a text editor, name it whatever but make sure to give it the `.sh` extension.

Paste this into the text file:

```bash
#!/bin/bash

url=$1

if [ ! -d "$url" ];then
				mkdir $url
fi

if  [ ! -d "$url/recon" ];then
				mkdir $url/recon
fi

echo "[+] Harvesting subdomains with assetfinder...."
assetfinder $url >> $url/recon/assets.txt
cat $url/recon/assets.txt | grep $1 >> $url/recon/final.txt
rm $url/recon/assets.txt
```

So what this script does is, it runs assestfinder, saves the results to a file `assets.txt` in the `recon` directory, if there’s no such directory, it will make one. Then finally it sorts the results from `assets.txt` into a `final.txt` file and deletes the former.

Save it, give it execute permissions with `chmod +x` and run it `./file.sh <URL>`:

Now list out the current directory and there should be a `recon` directory containing our `final.txt` file with fully sorted subdomains!! Nice.

### III. Finding Subdomains with Amass

Another great tool for subdomain hunting is Amass: 

[https://github.com/owasp-amass/amass](https://github.com/owasp-amass/amass)

Install with go using the following command:

```jsx
go install -v github.com/owasp-amass/amass/v4/...@master
```

Once that’s done, enumerate with `amass enum -d <URL>`:

![amass 1.png](20%20Web%20Application%20Enumeration,%20Revisited%20bfecbe21ba6b4faeb558d14d84e57a30/amass_1.png)

While that’s running, let us update the previous script by adding this below:

```bash
echo "[+] Harvesting subdomains with Amass...."
amass enum -d $url >> $url/recon/amass.txt
sort -u $url/recon/amass.txt >> $url/recon/amss_final.txt
rm $url/recon/amass.txt
```

Now we have an updated script that runs amass and saves the results into the recon folder.

### IV. Finding Alive Domains with Httprobe

Find the tool here: 

[https://github.com/tomnomnom/httprobe](https://github.com/tomnomnom/httprobe)

Install using: 

```bash
go install github.com/tomnomnom/httprobe@latest
```

We can pair this tool with the results from the `final.txt` file to check which subdomains are responding while removing the https and port number:

```bash
cat <URL>/recon/final.txt | httprobe | sed 's/https\?:\/\///' | tr -d ':443'
```

![httprobe 1.png](20%20Web%20Application%20Enumeration,%20Revisited%20bfecbe21ba6b4faeb558d14d84e57a30/httprobe_1.png)

Nice. Now let’s add this feature to our script.

```bash
echo "[+] Probing for alive domains...."
cat $url/recon/final.txt | sort -u | httprobe -s -p https:443  | sed 's/https\?:\/\///' | tr -d ':443' >> $url/recon/alive.txt

```

Now when we run our script we should have two files `final.txt` which contains sorted subdomains and `alive.txt` which uses httprobe to only list out active domains.

![httprobe 2.png](20%20Web%20Application%20Enumeration,%20Revisited%20bfecbe21ba6b4faeb558d14d84e57a30/httprobe_2.png)

## Screenshotting Websites with GoWitness

GoWitness is a tool that takes screnshots of websites. What makes this useful is, instead of copying the IP of every domain and pasting it in the browser to see what it looks it. This tool can automate that. Find the tool here:

[https://github.com/sensepost/gowitness](https://github.com/sensepost/gowitness)

Install using the following command:

```bash
go install github.com/sensepost/gowitness@latest
```

After that’s complete, let’s take a single screenshot using the following command:

```bash
gowitness single <URL>
```

A `screenshots` folder should be created with a screenshot of the URL we provided.

![gowitness.png](20%20Web%20Application%20Enumeration,%20Revisited%20bfecbe21ba6b4faeb558d14d84e57a30/gowitness.png)

## Automating the Enumeration Process:

sumrecon: [https://github.com/thatonetester/sumrecon](https://github.com/thatonetester/sumrecon)

TCM's modified script - [https://pastebin.com/MhE6zXVt](https://pastebin.com/MhE6zXVt)

## Additional Resources:

The Bug Hunter's Methodology - [https://www.youtube.com/watch?v=uKWu6yhnhbQ](https://www.youtube.com/watch?v=uKWu6yhnhbQ)

Nahamsec Recon Playlist - [https://www.youtube.com/watch?v=MIujSpuDtFY&list=PLKAaMVNxvLmAkqBkzFaOxqs3L66z2n8LA](https://www.youtube.com/watch?v=MIujSpuDtFY&list=PLKAaMVNxvLmAkqBkzFaOxqs3L66z2n8LA)